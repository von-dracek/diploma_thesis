\chapter{Reinforcement learning}
Reinforcement learning is a machine learning paradigm inspired by the natural learning process of humans -- learning by interacting with an environment. All actions we take in our daily lives are in some way punished or rewarded. For an example, consider touching a hot stove. An immediate negative reward (pain) is received and one learns quickly not to do it again. On the other hand, eating something sweet usually produces a feeling of pleasure (positive reward) and that makes us want to eat more sweets. Reinforcement learning methods work in pretty much the same way: an agent is placed in an artificial environment and based on the actions it takes, it receives rewards (positive or negative) and learns to perform the actions that yield the most positive rewards. This is in constrast to the other machine learning paradigms (supervised and unsupervised learning), where no environment exists and the model is learned by minimising some kind of loss over a given dataset. 

In the recent years, machine learning has seen a large surge in activity due to rising computational power and this has not avoided the field of reinforcement learning. Many large institutions and corporations have built teams that specialise in reinforcement learning and have produced groundbreaking results in many disciplines, ranging from beating the best player in the world in the game of Go (see \cite{alphago_paper}), solving the protein folding problem (see \cite{alphafold}), beating some of the best teams in Dota 2 (see \cite{openaifive}) or most recently, finding a faster matrix multiplication algorithm that current state of the art (see \cite{matrix_multiplication}).

In this chapter, we aim to provide the necessary exposition of reinforcement learning methods used in the computational part of this thesis. We mainly follow \cite{sutton2018reinforcement}.