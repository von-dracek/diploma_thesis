\chapter{Reinforcement learning}
\label{chapter3}
Reinforcement learning is a machine learning paradigm inspired by the natural learning process of humans -- learning by interacting with an environment. All actions we take in our daily lives are in some way punished or rewarded. 
	
For an example, consider touching a hot stove. An immediate negative reward (pain) is received and one learns quickly not to do it again. On the other hand, eating something sweet usually produces a feeling of pleasure (positive reward) and that makes us want to eat more sweets. 

Reinforcement learning methods work in pretty much the same way: an agent is placed in an artificial environment and based on the actions it takes, it receives rewards (positive or negative) and learns to perform the actions that yield the most positive rewards. This is in constrast to the other machine learning paradigms (supervised and unsupervised learning), where no environment exists and the model is taught by minimising some kind of loss over a given dataset. 

In the recent years, machine learning has seen a large surge in activity due to rising computational power and this has not avoided the field of reinforcement learning. Many large institutions and corporations have built teams that specialise in reinforcement learning and have produced groundbreaking results in many disciplines, ranging from beating the best player in the world in the game of Go (see \cite{alphago_paper}), solving the protein folding problem (see \cite{alphafold}), beating some of the best teams in Dota 2 (see \cite{openaifive}) or most recently, finding a faster matrix multiplication algorithm than current state of the art (see \cite{matrix_multiplication}).

In this chapter, we aim to provide the necessary exposition of reinforcement learning methods used in the computational part of this thesis. We mainly follow \cite{sutton2018reinforcement}.

\section{Basic definitions}
As we mentioned, the agent operates in an environment. The agent is aware of the environment and based on the current state of the environment chooses an action. The action is usually chosen as the action that maximises the expected cumulative reward (this may not always be the case, as choosing a less optimal action might be beneficial, we will touch on this in Section \ref{section:explo_vs_exploit}). After the agent performs an action, the state of the environment changes and the agent receives a reward. This happens in a sequence of discrete timesteps until a terminal state of the environment is reached (i.e., if the last chosen action led to winning/losing in the game of chess). One iteration of solving an environment from the initial to the terminal state is called an \textit{episode}. This is best illustrated by the chart that can be found in almost all reinforcement learning books, see Figure \ref{fig:agent_environment_interaction}.

\begin{figure}
  \includegraphics[width=\linewidth]{../img/agent_environment_interaction.png}
  \caption{Illustration of the agent environment interaction. Agent is in state $S_t$ and received reward $R_t$ for the last chosen action $A_{t-1}$, then $A_t$ is chosen by the agent and new state $S_{t+1}$ and reward $R_{t+1}$ are obtained. Image sourced from \cite[Figure 3.1.]{sutton2018reinforcement}.}
  \label{fig:agent_environment_interaction}
\end{figure}
\todo{should I write here explicitly that $S_t$, $A_t$ and $R_t$ are random variables?}

We now introduce the notion of a \textit{Markov decision process}, which is a formalisation of the agent environment interaction discussed above. The following is heavily inspired by \cite[Chapter 3]{sutton2018reinforcement}
\begin{defn}{Markov decision process} 
\label{defn:markov_decision_process}
\\
\cite[Chapter 3]{sutton2018reinforcement}

A Markov decision process is a 5-tuple $(\mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma)$, where 
\begin{enumerate}
\item $\mathcal{S}$ is the set of all possible states,
\item $\mathcal{A}$ is the set of all possible actions,
\item $\mathcal{P}(s_{t+1}|s_{t},a_t)=P(S_{t+1}=s_{t+1}|S_t=s_{t},A_t=a_t)$ is the probability that choosing action $a_t$ in state $s_t$ yields state $s_{t+1}$,
\item $\mathcal{R}(s_t, a_t)=R_{t+1}(S_t=s_t, A_t=a_t)$ is the reward received by choosing action $a_t$ in state $s_t$,
\item $\gamma \in [0,1)$ is the discount factor,

\end{enumerate}
\end{defn}
where $S_t$, $A_t$ and $R_t$ are random variables representing the state, action and reward at timestep $t$. The name Markov decision process is not a coincidence and is related to the Markovian property. Notice that the state transition probabilities depend only on previous state and chosen action and not on the preceeding history. This means that the state must contain all information and no information is carried by the previously visited states and taken actions. Of course, this is a simplification and does not hold in real life (for example, the history of moves can hold information about the strength of an opponent in the game of chess), but it suffices to model even complex phenomena and allows for precise mathematical treatment. 

The agent chooses an action based on the current state to maximise the expected cumulative reward. A function that maps states to probabilities of actions is termed \textit{policy}, see Definition \ref{defn:policy}.

\begin{defn}{Policy} \label{defn:policy}
\\
\cite[Section 3.5]{sutton2018reinforcement} \\
Let $s \in \mathcal{S}$ and $a \in \mathcal{A}$. Then the policy is defined as
\begin{equation*}
\pi(a|s)=P(A_t=a|S_t=s).
\end{equation*}
\end{defn} 
Another fundamental concept is the value function, see Definition \ref{defn:value_function}.

\begin{defn}{Value function} \label{defn:value_function}
\\
\cite[Section 3.5]{sutton2018reinforcement}
\\
Let $\pi$ be a policy and $s \in \mathcal{S}$. Then we define the value function $v_{\pi}(s)$ as
\begin{equation*}
v_{\pi}(s)=\mathbb{E}_{\pi} \left[ \sum_{k=0}^{\infty} \gamma^k R_{t+k+1} | S_t=s \right],
\end{equation*}
\end{defn}
where the subscript $\pi$ in $\mathbb{E}_{\pi}$ refers to the fact that the agent acts according to policy $\pi$.
The value function assigns each state the expected cumulative discounted reward -- the reward that the agent may expect to gain from state $s_t$ into the future. The discount factor $\gamma$ weights the future rewards by how far into the future they may be attained.

Another related notion is the \textit{action value function}, see Definition \ref{defn:action_value_function}.

\begin{defn}{Action value function} \label{defn:action_value_function}
\\
\cite[Section 3.5]{sutton2018reinforcement}
\\
Let $\pi$ be a policy and $s \in \mathcal{S}$ and $a \in \mathcal{A}$. Then we define the action value function $q_{\pi}(s,a)$ as
\begin{equation*}
 q_{\pi}(s,a)=\mathbb{E}_{\pi} \left[ \sum_{k=0}^{\infty} \gamma^k R_{t+k+1} | S_t=s, A_t=a \right].
\end{equation*}
\end{defn}
The action value function is much the same as value function, but it maps the actions taken in a state to the expected cumulative discounted reward rather than just states. The action value function allows the agent to not always take the immediate most rewarding (greedy) action, but rather optimise the reward while taking into account the possible following states and actions. 

The value function and action value functions must be learned by exploring the environment. Unfortunately, the plethora of theory about estimation of these functions (e.g. using the Bellman equations) is out of scope of this thesis. We refer the interested reader to \cite[Section 3.5.]{sutton2018reinforcement} and the references therein.

Another related notion, which builds on the definitions given above is the \textit{advantage function}, see Defintion \ref{defn:advantage_function}.
\begin{defn}{Advantage function} \label{defn:advantage_function}
\\
\cite[Section 3]{a3c_paper}
\\
Let $\pi$ be a policy and $s \in \mathcal{S}$ and $a \in \mathcal{A}$. Denote the value function as $v_{\pi}(s)$ and the action value function as $q_{\pi}(s,a)$. Then we define the advantage function  as
\begin{equation*}
 A_{\pi}(s,a) = q_{\pi}(s,a) - v_{\pi}(s).
\end{equation*}
\end{defn}
The advantage represents the gain that is obtained by taking action $a$ in state $s$ compared to the average action.

Our aim is to obtain a policy that maximises the expected cumulative discoutned reward. We thus define the \textit{optimal policy}, \textit{optimal value function} and \textit{optimal action value function}, see Definition \ref{defn:optimal_definitions}.

\begin{defn}{Optimal policy, value function and action value function} 
\label{defn:optimal_definitions} 
\\
\cite[Section 3.6]{sutton2018reinforcement}
\\
Let $R(\pi)$ be the expected cumulative discounted reward obtained by following policy $\pi$. Then we define the optimal policy as
\begin{equation*}
 	\pi^* = \underset{\pi}{argmax} \, R(\pi).
\end{equation*}
Similarly, the optimal value function and optimal action value function are given by: 
\begin{equation*}
 	v^*(s) = \underset{\pi}{\max} \, v_{\pi}(s)
\end{equation*}
and
\begin{equation*}
 	q^*(s,a) = \underset{\pi}{\max} \, q_{\pi}(s,a).
\end{equation*}
respectively for $s \in \mathcal{S}$ and $a \in \mathcal{A}$.
\end{defn}

\section{Exploration vs exploitation}
\label{section:explo_vs_exploit}
The optimal policy, value function and action value function must be learned by interacting with the environment. The agent now faces a dillema -- either to maximise his known reward (act greedily, but potentially get stuck in a local optimum) or explore the environment and update the policy in order to get the globally optimal policy. This exploration-exploitation tradeoff is always present with reinforcement learning and many approaches for dealing with it exist. An example are the $\epsilon$-greedy methods, where the agent acts greedily $1-\epsilon \, \%$ of the time and performs a random action $\epsilon \, \%$ of the time. $\epsilon$ is usually set to a value close to 1 at the beginning of training and decreased over time, the final threshold at which $\epsilon$ is kept constant is usually somewhere in $[0,0.1]$. For more information, we refer the reader to \cite[Section 2.7.]{sutton2018reinforcement}.

\section{Algorithm classes}
In this section, we aim to present a basic summary of current reinforcement learning methods with particular focus on policy-gradient methods.
\subsection{Model free vs. model based algorithms}
The most fundamental dividing line between reinforcement learning algorithms is whether the agent is given a model of the environment which allows the agent to take into account future states before they are experienced. The model is represented by the state transition function $\mathcal{P}$ as defined in Definition \ref{defn:markov_decision_process}. Having the model in hand obviously helps the agent learn tremendously, but having such a model in practice is quite rare, thus the model free methods are being used much more extensively. We focus on model free algorithms in this text.

\subsection{Model free methods}
In comparison to model based methods, the model free methods learn by trial and error.
Examples of these methods are e.g. Monte Carlo Sampling, SARSA, Q-learning, Actor critic, Proximal policy optimization (PPO) and Trust region policy optimization (TRPO). These methods can be divided into two groups -- value based methods and policy based methods. In this thesis, we focus particularly on the Proximal policy optimization method (as it will be used in the computational part of this thesis for reasons explained later), but we also give an introduction to Q learning, as it helps with understanding of how deep neural networks are used in the field of reinforcement learning, the Actor critic architecture and the Trust region policy optimization. We assume that the reader is familiar with basics of deep learning (such as architecture of neural networks and basic algorithms for training them such as stochastic gradient descent), a great introduction can be found in \cite[Part 2]{deep_learning_book}.
\subsubsection{Value based methods}
\subsubsection{$Q$ learning}
In this section, we follow \cite[Section 6.5.]{sutton2018reinforcement}.
The most famous example of a value based method is $Q$ learning.
Let $q^*(s,a)$ denote the optimal action value function as defined in Definition \ref{defn:optimal_definitions} and let $Q(s,a)$ be an estimate of $q^*(s,a), s \in \mathcal{S}, a \in \mathcal{A}$. If the sets $\mathcal{S}$ and $\mathcal{A}$ are finite, then the values of $Q(s,a)$ can be represented by a table and updated according to the updating rule presented in Equation \ref{eq:q_learning_updating_rule}.

\begin{equation}
Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left[r_{t+1} + \gamma \underset{a \in \mathcal{A}}{\max}Q(s_{t+1}, a) - Q(s_t, a_t) \right],
\label{eq:q_learning_updating_rule}
\end{equation}
where $\alpha$ is the learning rate, $\gamma$ is the discount factor, $s_t, a_t$ are the current state and current chosen action respectively, $s_{t+1}$ is the next state following action $a_t$, $r_{t+1}$ is the reward obtained by choosing action $a_t$ and the subscript $t$ is added to emphasize the transition between current and next step.
The whole algorithm can be summarised as follows:

\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}


\begin{algorithm}[H]
\small
\caption{Vanilla Q-learning \cite[p. 131]{sutton2018reinforcement}}\label{alg:q_learning}
\begin{algorithmic}
\Require{Choose the learning rate $\alpha \in (0,1]$ and exploration parameter $\epsilon>0$, number of episodes, initialise $Q(s, a) \, \mathrm{randomly}, s \in \{s \, \mathrm{for} \,  s \, \mathrm{in} \, \mathcal{S} \, \mathrm{if} \, s \, \mathrm{is \, not \, terminal}\}$ and $Q(s,a)=0$ for $s$ terminal, $a \in \mathcal{A}$ (terminal state of the environment means that the episode is finished).}
\For{episode in $\{$1,...,number of episodes$\}$}
\State $s_t \leftarrow s_0$ Reset environment state 
\While{$s_t$ is not terminal}
\State $a_t \leftarrow$ Choose action $a_t$ using an epsilon-greedy policy according to $Q(s_t,\cdot).$
\State $s_{t+1}, r_{t+1} \leftarrow$ act on action $a_t$, obtain new state $s_{t+1}$ and reward $r_{t+1} $
\State Update $Q(s_t,a_t)$ using Equation \ref{eq:q_learning_updating_rule}
\State $s_t \leftarrow s_{t+1}$
\EndWhile
\EndFor
\end{algorithmic}
\end{algorithm}
It has been shown that under the assumption that all state-action pairs continue to be updated during training, then $Q$ converges to $q^*$ almost surely \cite[p. 131]{sutton2018reinforcement}. This variant of $Q$-learning has an obvious problem -- it depends on a table for keeping the values of the $Q$ function and thus does not generalise to complex state spaces (such as infinite ones). To tackle this problem, a neural network has been introduced in place of the $Q(s,a)$ table as a function approximator, which we can then write as $Q(s,a;\theta)$, where $\theta$ are the weights of the neural network. This approach has been popularised by \cite{deep_q_learning_paper}, where they used $Q$-learning along with a neural network as a function approximator (and many other particular improvements such as experience replay, that are unfortunately out of the scope of this thesis) to achieve superhuman performance on several Atari 2600 games. The reader interested in deep $Q$-learning can find the algorithm in \cite[Algorithm 1]{deep_q_learning_paper}.

\subsubsection{Policy approximation methods}
The theory of value based methods assumed that we estimate the value function and action value function and then based on them somehow choose the policy (such as taking the action with maximum value). However, another approach is possible -- modelling the policy explicitly. In this section, we mostly follow  \cite[Chapter 13]{sutton2018reinforcement} and \cite[Section 2]{proximal_policy_optimization}.

Policy approximation methods assume that the policy $\pi(a|s;\theta), a \in \mathcal{A}, s \in \mathcal{S}$ is dependent on parameters $\theta \in \R^d$ where $d \in \N.$ In general, the aim of these methods is to maximise some kind of performance measure $J(\theta)$. The performance measure $J$ is chosen such that the gradient $\nabla_{\theta}J$ exists and by estimating the gradient as $\widehat{\nabla_{\theta_{\tau}} J(\theta_{\tau})}$, the policy can be optimised using stochastic gradient ascent (where the subscript $\tau$ implies that this estimate is computed in the $\tau$-th update in the stochastic gradient ascent process). In practice, $\widehat{\nabla_{\theta_{\tau}} J(\theta_{\tau})}$ is estimated by averaging a batch of samples, we will denote this as $\mathbb{E}_b \widehat{\nabla_{\theta_{\tau}} J(\theta_{\tau})}$ where the symbol $\mathbb{E}_b$ refers to the average of a batch of samples. We can then write the very general update rule

%Equation \ref{eq:policy_approximation_updating_rule}
%
\begin{equation*}
\label{eq:policy_approximation_updating_rule}
\theta_{\tau+1}=\theta_{\tau}+\alpha \mathbb{E}_b\widehat{\nabla_{\theta_{\tau}} J(\theta_{\tau})},
\end{equation*}
where $\alpha$ is the learning rate.
% and $\widehat{\nabla_{\theta_{\tau}} J(\theta_{\tau})}$ is the estimate of the gradient of the performance measure $J$ with respect to $\theta_{\tau}$. 
Due to the aforementioned updating rule, these methods are also often called \textit{policy gradient methods}. 

\begin{rem} 
When using policy approximation methods, we do not need to randomly sample actions $\epsilon \%$ of the time to ensure exploration. All that is needed is to ensure that the policy does not become deterministic. This can be achieved by ensuring that $\pi(a|s;\theta) \in (0,1)$ \cite[Section 13.1]{sutton2018reinforcement}.
\end{rem}

A significant theoretical advantage compared to value based methods is that “\textit{with continuous policy parameterization the action probabilities change smoothly as a function of the learned parameter, whereas in $\epsilon$-greedy selection the action probabilities may change dramatically for an arbitrarily small change in the estimated action values, if that change results in a different action having the maximal value. Largely because of this stronger convergence guarantees are available for policy-gradient methods than for action-value methods}” \cite[Section 13.2]{sutton2018reinforcement}. We refer the reader to the Policy gradient theorem located therein.

There exist also hybrid methods between policy gradient methods and value based methods, where the policy, value function and action value function are all learned -- such methods are called \textit{actor critic} methods.

\subsubsection{Advantage actor critic}
In this section, we present the theory behind the (asynchronous) advantage actor critic (A3C) algorithm as developed in \cite{a3c_paper}, as it shows well the structure of the neural net that is used in the PPO algorithm.

The advantage actor critic is a policy approximation algorithm that learns not only the policy but also the action value function. Let us first decipher the name of the algorithm. Advantage refers to the advantage function defined in Definition \ref{defn:advantage_function}, actor refers to the learned policy approximation and critic refers to the learned action value function approximation (both the actor and the critic are neural networks that are used as function approximators). In practice, the actor and critic networks share some parameters (they can be thought of as a single neural net with diverging structure, such as a first shared layer and then diverging such that the second layer is not shared between the actor and the critic).

Denote the weights of the actor as $\theta$ and weights of the critic as $\theta_v$, thus the estimated policy can be written as $\pi(a|s;\theta)$, $a \in \mathcal{A}, s \in \mathcal{S}$. The performance measure $J$ that A2C maximises can be written as 
\begin{equation*}
\log (\pi(a_t|s_t;\theta))\widehat{A}(s_t,a_t;\theta,\theta_v),
\end{equation*}
and the gradient of the performance measure can be written as
\begin{equation*}
\nabla_{\theta'} \log (\pi(a_t|s_t;\theta'))\widehat{A}(s_t,a_t;\theta,\theta_v),
\end{equation*}
where the gradient is taken only with respect to the actor variables affecting the policy (the advantage can be thought of as constant with regard to the differentiation) and where we again add the time subscript $t$ and where $\widehat{A}(s_t,a_t;\theta,\theta_v)$ is the estimated advantage function (for details on how it can be estimated, see \cite[Section 4]{a3c_paper}). Note that this update only updates the policy and not the value function. A different updating scheme is used for the parameters of the critic, where a quadratic error between the estimated value function and observed rewards is minimised, the details ca be found in the original paper \cite[Algorithm S3]{a3c_paper}.
% given by 
%\begin{equation}
%\widehat{A_t}(s_t,a_t;\theta,\theta_v)=\sum_{i=0}^{k-1}\gamma^i r_{t+1} + \gamma^k V(s_{t+k};\theta_v) - V(s_{t};\theta_v),
%\end{equation}
%where $\gamma$ is the discount factor and where $k$ is the number of steps the agent has taken before the update, which is upper bounded by a chosen hyperparameter $t_{max}$. $t_{max}$ chooses at most how many steps the agent should take in an environment. 
%The update is performed whenever a final state is reached or when $t_{max}$ steps have been taken.
%\begin{rem}
%Notice that in the formula for $\widehat{A_t}(s_t,a_t;\theta,\theta_v)$ the parameters $\theta$ are not explicitly present. In reality, the rewards $r_{(\cdot)}$ are chosen by the policy and thus depend on the parameters $\theta$. \todo{is this remark correct? please check} 
%\end{rem}

In the original paper, they made use of asynchronous updates to supposedly improve training stability. It was later shown in the paper \cite{a3c_asynchrony_not_necessary} that the asynchronicity provides no added benefit in performance.

\subsection{Trust region policy optimization}
The trust region policy optimization method, developed in \cite{TRPO}, is a special case of policy gradient methods, as it introduces a special constraint on the policy parameters, such that the change in the policy is not too large at each step. This is done by imposing the constraint that the Kullback-Leibler Divergence between the two policies is not too large. The Kullback-Leibler Divergence is defined in Definition \ref{defn:kl_divergence}.
\begin{defn} \label{defn:kl_divergence}
{Kullback-Leibler Divergence}
\\
Let $P$ and $Q$ be discrete random variables with the same support $\mathcal{S}$. Let $P(x)$ and $Q(x)$ denote the probability distribution functions of $P$ and $Q$, $x \in \mathcal{S}$. Then the Kullback-Leibler Divergence, denoted $D_{KL}(P,Q)$ is calculated is
\begin{equation*}
D_{KL}(P,Q) = \mathbb{E}_{P} \log(\frac{P}{Q}) = \sum_{x \in \mathcal{S}} P(x) \log(\frac{P(x)}{Q(x)}),	
\end{equation*}
where the subscript $P$ in $\mathbb{E}_{P}$ denotes that the expectation is taken with respect to the probability distribution of the random variable $P$.
\end{defn}
$D_{KL}$ quantifies the difference between discrete probability distributions. More details can be found in \cite{KL_divergence}.

In practice, $D_{KL}(\pi_{A},\pi_{B})$ between two policies $\pi_{A}$ and $\pi_{B}$ is bounded from above using some parameter $\delta$. At each step, the optimization problem that is being solved to update the policy is given in Equation \ref{eq:TRPO}.
\begin{alignat}{10}
&  \underset{\theta_{\tau+1}}{\max} \, && \mathbb{E}_b \frac{\pi(a_t, s_t|\theta_{\tau+1})}{\pi(a_t, s_t|\theta_{\tau})} \widehat{A}(a_t, s_t) \label{eq:TRPO} \\
s.t. & && \mathbb{E}_b D_{KL}(\pi(\cdot, s_t|\theta_{\tau}), \pi(\cdot, s_t|\theta_{\tau+1})) \leq \delta \nonumber,
\end{alignat}
where $\theta_{\tau+1}$ are the new parameters of the policy after the update and $\widehat{A}(a_t, s_t)$ is an estimate of the advantage function.
The objective function that is maximised here is a local approximation of a quantity that \textit{“represents the expected return
of another policy $\pi(\cdot,\cdot|\theta_{\tau+1})$ in terms of the advantage over $\pi(\cdot,\cdot|\theta_{\tau})$”} \cite[Equation 1]{TRPO}. For details (such as how $\widehat{A}(a_t, s_t)$ is calculated\footnote{which we do not show, as it would require developing needlessly complex notation and it is not particularly relevant for our purposes}), see \cite[Section 2-4]{TRPO}.

\subsection{Proximal policy optimization}
The Proximal policy optimization algorithm (PPO) was developed in \cite{proximal_policy_optimization} by combining a neural network used for estimation of action value function and the policy approximation with the trust region idea used in TRPO, we follow their exposition. Consider Equation \ref{eq:ppo}.
\begin{equation}
\label{eq:ppo}
\footnotesize	
 L_{CLIP}(\theta_{\tau+1}, \theta_{\tau}) = \min(r_t(\theta_{\tau+1},\theta_{\tau})\widehat{A}(a_t, s_t), \, \mathrm{clip}(r_t(\theta_{\tau+1},\theta_{\tau}), 1-\delta, 1+\delta)\widehat{A}(a_t, s_t))),
\end{equation}
where $r_t(\theta_{\tau+1},\theta_{\tau}) = \frac{\pi(a_t, s_t|\theta_{\tau+1})}{\pi(a_t, s_t|\theta_{\tau})}$, $\delta$ is a hyperparameter and 
\begin{equation*}
\mathrm{clip}(a, 1-\delta, 1+\delta) = \min(1+\delta, \max(a,1-\delta)), a \in R.
\end{equation*}

The first term in Equation \ref{eq:ppo} is the same as in the objective function of the TRPO method, see Equation \ref{eq:TRPO}. The second term in the minimum clips the ratio $r_t(\theta_{\tau+1},\theta_{\tau})$, which  has the effect of limiting the change of the policy so that the change is not too large (controlled by $\delta$). The minimum in Equation \ref{eq:ppo} is then taken to get a lower bound on the same objective as used in TRPO.

$L_{CLIP}(\theta_{\tau+1}, \theta_{\tau})$ is then combined with a value function error term $L_{VF}(\theta_{\tau+1})$ and potentially also an entropy term (which is omitted in this exposition), giving rise to the following performance measure $L$ as given in \ref{eq:ppo_performance_measure}.

\begin{equation}
\label{eq:ppo_performance_measure}
L(\theta_{\tau+1}, \theta_{\tau}) =  L_{CLIP}(\theta_{\tau+1}, \theta_{\tau}) - c_1 L_{VF}(\theta_{\tau+1}),
\end{equation}
where $c_1$ is a hyperparameter and $L_{VF}(\theta_{\tau+1})=(v_{\theta_{\tau+1}}(s_{t_k})-v_{t_k}^{target})^2$, where $v_{t_k}^{target}$ is the observed cumulative return of the state $s_{t_k}$ obtained during training, $k=1,\dots,M,$ where $M$ is the batch size. This performance measure is then maximised using stochastic gradient ascent (note that for Equation \ref{eq:ppo_performance_measure} to make sense, particularly regarding the value of $L_{VF}(\theta_{\tau+1})$ which gives the error for a single state, the stochastic gradient ascent uses $\mathbb{E}_b L(\theta_{\tau+1}, \theta_{\tau})$, which is the average over a batch of samples of size $M$). Note that while the parameters $\theta_{\tau}$ are shared for the policy approximation and the value function approximation $v_{\theta_{\tau}}$, a similar shared neural net architecture as was used in the actor critic framework can be used here such that only some of the parameters are shared (such as a common first layer). Particularly, A2C is a special case of PPO as was shown in \cite{a2c_ppo_special_case}. 
\todo{This last part is very rough - I am not really sure if it is written clearly. The literature does not give a very precise treatment to this particular part.} 

Empirically, PPO performs better than A2C and TRPO and is more sample efficient (requires less timesteps to reach a given level of performance). Particularly, in a blogpost, OpenAI said that \textit{“it has become the default reinforcement learning algorithm they use due to its ease of use and good performance”}, see \cite{openaiblogpost}.

%The PPO algorithm, written exactly as originally written in \cite{proximal_policy_optimization}, then reads:
%
%\begin{algorithm}[H]
%\small
%\caption{PPO, Actor Critic style \cite[Algorithm 1]{proximal_policy_optimization}} \label{alg:ppo}
%\begin{algorithmic}
%\For{iteration in 1,2,... do}
%	\For{actor in 1,2,...,$N_A$ do}
%		\State Run policy $\pi(\cdot, \cdot|\theta_t)$ in environment for $T$ timesteps
%		\State Compute advantage estimates $\widehat{A}_1,...,\widehat{A}_T$
%	\EndFor
%	\State Optimize $L(\theta_{t+1}, \theta_{t})$ with respect to $\theta_{t+1}$, with $K$ epochs and minibatch 	    \State size $M \leq N_AT$
%	\State $\theta_t \leftarrow \theta_{t+1}	$
%\EndFor
%\end{algorithmic}
%\end{algorithm}

