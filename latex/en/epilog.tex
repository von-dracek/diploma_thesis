\chapter*{Conclusion}
\addcontentsline{toc}{chapter}{Conclusion}
In this thesis, we explored the dependence of multistage scenario models on the chosen scenario tree structure.

We explored the dependence of the objective value on the scenario tree structure and we have shown that the structure of scenario trees that is usually used in practice may not always be the best. We found out that there are significant differences in the obtained rewards when using a different number of stages, while the differences between different scenario trees with a fixed number of stages are not that staggering.

Further we proposed an experiment to explore the dependence of the value of the objective function of the mean-CVaR model on the structure of a scenario tree that was built using the moment matching method from historical data using reinforcement learning. Further, we trained several reinforcement agents and evaluated their performance, finding that it is possible to train such an agent to aid in the task of choosing a scenario tree structure and we have shown that for 3 and 5 stage trees, the structure of the scenario trees chosen by the agent is rather different compared to the structure usually used in practice. We further explored the effect of including a penalty for choosing a scenario tree structure that is too complex. 

This thesis could be extended in several ways. Due to computational limitations, we had to constrain ourselves only to a small set of trees. It would be interesting to extend the experiment such that more stages and more branchings are allowed. Moreover, other scenario tree building methods than moment matching could be explored. It would be also very interesting to train the agent for a lot longer than we did, as we cannot be sure that a significantly longer training would not lead to better results. All of these extensions would however come at a significant computational cost with the current implementation.